{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szbbuf-zWL0m"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJ161QqDWL05"
   },
   "source": [
    "# Семинар – Деревья решений "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOfsYfCRWL1B"
   },
   "source": [
    "## 1. Построение и визуализация деревьев решений\n",
    "Для нагляности в качестве простого примера, возьмем всем известные ирисы:\n",
    "\n",
    "__150 объектов__  \n",
    "__4 признака: __ \n",
    "- длина чашелистика (см)  \n",
    "- ширина чашелистика (см)  \n",
    "- длина лепестка (см)  \n",
    "- ширина лепестка (см)  \n",
    "\n",
    "__3 класса:__  \n",
    "- Ирис щетинистый (Iris setosa)  \n",
    "- Ирис виргинский (Iris virginica)  \n",
    "- Ирис разноцветный (Iris versicolor)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "589k1FsJWL1D"
   },
   "outputs": [],
   "source": [
    "# Загрузим данные\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:, 2:] # Возьмем только два признака: petal length и width\n",
    "y = iris.target\n",
    "\n",
    "target_names = iris.target_names\n",
    "feature_names = iris.feature_names[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yC0OsvGvWL1V"
   },
   "source": [
    "Мы не будем делить выборку на трейн/тест, а для демонстрации просто построим решающее дерево на всех объектах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njstBb3gWL1a"
   },
   "source": [
    "### Построение дерева решений\n",
    "В sklearn за это отвечают `DecisionTreeClassifier` (для классификации) и `DecisionTreeRegressor` (для регрессии) из модуля `tree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06l7k5OdWL1l"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TKvvsJbtpGcR"
   },
   "source": [
    "Обучите свое первое дерево классификации с параметрами: `max_depth=2` и `random_state=42`, в качестве данных для обучения используйте X и y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jg6qxpQiWL11"
   },
   "outputs": [],
   "source": [
    "tree_clf = # <Ваш код здесь>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UO4khiZWL2p"
   },
   "source": [
    "Мы ограничили глубину дерева (за это овечает параметр `max_depth`), подробнее о том, для чего мы это сделали поговорим позже.  \n",
    "  \n",
    "Кроме того, не забываем фиксировать `random_state` для воспроизводимости результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ooiel30FWL2r"
   },
   "source": [
    "### Визуализация обученного дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcSWrQZRWL2u"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LCpVsk9UWL29"
   },
   "outputs": [],
   "source": [
    "# Отрисуем дерево\n",
    "export_graphviz(tree_clf, feature_names=feature_names,\n",
    "                class_names=target_names, \n",
    "                out_file='iris_tree.dot', filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1qAoGM9Ih5dG"
   },
   "source": [
    "Мы будем использовать https://dreampuf.github.io/GraphvizOnline/ для отрисовки. \n",
    "\n",
    "Однако, с помощью библиотеки pydot (pip install pydot) и например команды: \n",
    "`!dot -Tpng 'iris_tree.dot' -o 'iris_tree.png'` можно перевести .dot в изображение и вывести дерево прямо в ноутбуке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zLvPD8VgWL3K"
   },
   "source": [
    "По умолчанию в sklearn используется критерий Gini, но его можно поменять с помощью параметра `criterion`\n",
    "\n",
    "__Напоминание:__\n",
    "$$G_i = 1 - \\sum_{k=1}^{n} {(p_{k})^2}$$\n",
    "$$G_{split} = \\frac {L}{N} \\times {G_L} + \\frac {R}{N} \\times {G_R} \\rightarrow min $$\n",
    "\n",
    "где:\n",
    "- $L$ - Количество элементов в левой ветке\n",
    "- $R$ -Количество элементов в правой ветке\n",
    "- $N$ - Количество элементов в узле  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DfShTxIRWL3L"
   },
   "source": [
    "__Подведем некоторые итоги:__\n",
    "1. Дерево легко интерпретируется \n",
    "2. И прирост информации, и критерий Джини очевидно обощаются на многоклассовый случай\n",
    "3. Дерево умеет предсказывать не только класс, но и вероятность отнесения к классу (с помощью метода `predict_proba`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9tZ9YOpWL3M"
   },
   "source": [
    "### Разделяющая поверхность\n",
    "Так как у нас всего два признака, давайте проследим, как строится разделяющие границы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g94xf2faWL3N"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n",
    "    # Создадим точки по осям\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    # Создадим координатную матрицу \n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    #Согласно предсказаниям модели, закрасим нужные нам границы, предикт для каждой точки сделаем прямо внутри\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    # Сделаем функцию более унивесальной, чтобы применятьне только к ирисам\n",
    "    if not iris:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    # Накидаем точек из Трейна\n",
    "    if plot_training:\n",
    "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\") # желтые (y) круги (o)\n",
    "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\") # синие (b) квадраты (s)\n",
    "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris-Virginica\") # зеленые (g) треугольники (^)\n",
    "        plt.axis(axes)\n",
    "    if iris:\n",
    "        plt.xlabel(\"Petal length\", fontsize=14)\n",
    "        plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "# Нарисуем границу предсказаний\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "\n",
    "# Явно нарисуем линии границ, получившегося ранее дерева \n",
    "plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n",
    "plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
    "plt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\n",
    "plt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\n",
    "plt.text(1.40, 1.0, \"Depth=0\", fontsize=15)\n",
    "plt.text(3.2, 1.80, \"Depth=1\", fontsize=13)\n",
    "plt.text(4.05, 0.5, \"(Depth=2)\", fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3Er3caoWL3t"
   },
   "source": [
    "Мы не будем останавливаться подробно на коде, а оставим это в качестве дополнительных материалов по matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXJWximoWL35"
   },
   "source": [
    "## 2. Переобучение и борьба с ним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXC9-Z_xWL39"
   },
   "outputs": [],
   "source": [
    "#Вернем все признаки в нашу выборку\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "target_names = iris.target_names\n",
    "feature_names = iris.feature_names\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDuFhhhrWL4C"
   },
   "source": [
    "Постройте Решающее дерево с параметрами по умолчанию (random_state=42)\n",
    "И отрисуйте его. \n",
    "На этот раз, мы не будем никак ограничивать наше дерево"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9RsjzvGWL4E"
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Vqdx4i_WL4M"
   },
   "source": [
    "Так как при построении дерева используется принцип жадной максимизации, то дерево достаточно легко переобучить\n",
    "\n",
    "\n",
    "### Объявляем борьбу с переобучением\n",
    "Основные параметры класса [sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html):\n",
    "\n",
    "- `max_depth` – максимальная глубина дерева\n",
    "- `max_features` - максимальное число признаков, по которым ищется лучшее разбиение в дереве (это нужно потому, что при большом количестве признаков будет \"дорого\" искать лучшее (по критерию типа прироста информации) разбиение среди *всех* признаков)\n",
    "- `min_samples_leaf` - минимальное число объектов в листе. У этого параметра есть понятная интерпретация: скажем, если он равен 5, то дерево будет порождать только те классифицирующие правила, которые верны как мимимум для 5 объектов\n",
    "- `min_samples_split` - Минимальное колличество объектов для разбиения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4X_I_Iwbu8Uh"
   },
   "source": [
    "### Разберем еще один пример\n",
    "Создадим игрушечный датасет make_moons (он действительно создает выборку два класса, по двум признкам в форме лун, которые входят одна в другу)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qk63exC_WL4N"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "#Создадим луны\n",
    "Xm, ym = make_moons(n_samples=100, noise=0.28, random_state=53)\n",
    "\n",
    "#Обучим Первый классификатор без ограничений \n",
    "deep_tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "deep_tree_clf1.fit(Xm, ym)\n",
    "\n",
    "#Обучим Второй классификатор ограничив число объектов в листе\n",
    "deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n",
    "deep_tree_clf2.fit(Xm, ym)\n",
    "\n",
    "# Нарисуем decision_boundary для каждой и модели\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n",
    "plt.title(\"No restrictions\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n",
    "plt.title(\"min_samples_leaf = {}\".format(deep_tree_clf2.min_samples_leaf), fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llODN18qWL4V"
   },
   "source": [
    "Поэкспериментируйте в примере Выше с параметрами (Правое дерево) и посмотрите как изменяется разделяющая поверхность. \n",
    "Попробуйте разные значения для каждого из параметров, а так же их комбинации:\n",
    "- `max_depth`\n",
    "- `min_samples_leaf`\n",
    "- `min_samples_split`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xvOYIBYCWL4X"
   },
   "source": [
    "## 3. Чувствительность дерева к выборкам\n",
    "Деревья решений очень чувствительны к положению данных в пространстве, так как строит разделяющую поверхность ортогонально относительно переменной. \n",
    "\n",
    "Рассмотрим еще один пример на игрушечных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56NTnOuXWL4a"
   },
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "# Создадим набор случайных точек с двумя признаками:\n",
    "Xs = np.random.rand(100, 2)*4 - 2\n",
    "# Целевая переменная будет просто разделяться по одному признаку в 0\n",
    "ys = (Xs[:, 0] > 0).astype(np.float32) * 2\n",
    "\n",
    "# На основе созданной выборки, создадим еще одну, просто путем поворота \n",
    "angle = np.pi / 4\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "Xsr = Xs.dot(rotation_matrix)\n",
    "\n",
    "# Для каждой выборки обучим дерево решений   \n",
    "tree_clf_s = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_s.fit(Xs, ys)\n",
    "tree_clf_sr = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_sr.fit(Xsr, ys)\n",
    "\n",
    "# Сравним их\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(tree_clf_s, Xs, ys, axes=[-2, 2, -2, 2], iris=False)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(tree_clf_sr, Xsr, ys, axes=[-2, 2, -2, 2], iris=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ru99WprtyIm1"
   },
   "source": [
    "В случае с повернутой выборкой, рещающая граница стала более сложной. \n",
    "Подумайте, какая модель справилась бы лучше с такой природой данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SYmlevi0WL4f"
   },
   "source": [
    "## Часть 4 - Регрессионые решающие деревья\n",
    "\n",
    " При прогнозировании вещественного признака идея построения дерева остается та же, но меняется критерий качества: \n",
    " \n",
    " - Дисперсия вокруг среднего: $$\\Large D = \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} (y_i - \\frac{1}{\\ell} \\sum\\limits_{i =1}^{\\ell} y_i)^2, $$\n",
    " где $\\ell$ – число объектов в листе, $y_i$ – значения целевого признака. Попросту говоря, минимизируя дисперсию вокруг среднего, мы ищем признаки, разбивающие выборку таким образом, что значения целевого признака в каждом листе примерно равны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAar328hWL4n"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtkeNB0fWL4t"
   },
   "source": [
    "Создадим выборку на основе зашумленной функции: \n",
    "$$y = 4(x-0,5)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jwHws_PoWL4v"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# Сгинерируем вектор X состоящий из 200 точек \n",
    "m = 200\n",
    "X = np.random.rand(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oumJHx1xy7l9"
   },
   "outputs": [],
   "source": [
    "#Создадим Y на основе нашей функции \n",
    "y = # Ваш код здесь\n",
    "# Добавим нормальный шум вокруг данных (чтобы шум был не такой сильный, разделим его на 10), поможет np.random.randn(200,1)\n",
    "y = # Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ftlyPDxWL43"
   },
   "source": [
    "### Обучим решающее дерево"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GiNgSCxSWL48"
   },
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BhY46TjnWL5P"
   },
   "source": [
    "### Визуализация и разбор процесса построения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iV3wBKJnWL5S"
   },
   "outputs": [],
   "source": [
    "# Обучим дерево решений ограничим его глубинной 2 (random_state=42)\n",
    "tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "# Обучим еще одно дерево, но теперь поставим ограничение на глбину 3 (random_state=42)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "tree_reg2.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLwPIqrM0Lkt"
   },
   "source": [
    "Визуализируем каждое из деревьев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFDkePjy0HoJ"
   },
   "outputs": [],
   "source": [
    "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.subplot(121)\n",
    "plot_regression_predictions(tree_reg1, X, y)\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "plt.text(0.21, 0.65, \"Depth=0\", fontsize=15)\n",
    "plt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\n",
    "plt.text(0.65, 0.8, \"Depth=1\", fontsize=13)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"max_depth=2\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "for split in (0.0458, 0.1298, 0.2873, 0.9040):\n",
    "    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\n",
    "plt.text(0.3, 0.5, \"Depth=2\", fontsize=13)\n",
    "plt.title(\"max_depth=3\", fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYDRs38GWL5f"
   },
   "source": [
    "### Проблема переобучения\n",
    "Регрессионное дерево имеет ту же природу, что и дерево классикации и строится жадно, поэтому имеет те же проблемы с переобучением "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgKhbTsPWL5g"
   },
   "outputs": [],
   "source": [
    "# Сравним, как будет выглядеть дерево без ограничений \n",
    "tree_reg1 = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "# и дерево в которое мы внесем ограничения\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "# Посмотрим, как каждая из моделей справляется с новыми данными\n",
    "x1 = np.linspace(0, 1, 500).reshape(-1, 1)\n",
    "# Сделаем предсказания: \n",
    "y_pred1 = tree_reg1.predict(x1)\n",
    "y_pred2 = tree_reg2.predict(x1)\n",
    "\n",
    "# Отрисуем результаты предсказаний\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18, rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"No restrictions\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTOGx-EsWL6i"
   },
   "source": [
    "## Заключение. Плюсы и минусы деревьев решений\n",
    "\n",
    "**Плюсы:**\n",
    " - Деревья решений легко визуализировать и интерпретировать;\n",
    " - Быстрые процессы обучения и прогнозирования;\n",
    " - Малое число параметров модели.\n",
    " \n",
    "**Минусы:**\n",
    " - Деревья очень чувствительны к шумам во входных данных, вся модель может кардинально измениться, если немного изменится обучающая выборка, поэтому и правила классификации могут сильно изменяться;\n",
    " - Разделяющая граница, построенная деревом решений, имеет свои ограничения (состоит из гиперплоскостей, перпендикулярных какой-то из координатной оси);\n",
    " - Необходимость отсекать ветви дерева (pruning) или устанавливать минимальное число элементов в листьях дерева или максимальную глубину дерева для борьбы с переобучением;\n",
    " - Жадный поиск признака с максимальным приростом информации не гарантируют нахождения глобально оптимального дерева;\n",
    " - Сложно поддерживаются пропуски в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqIAQomxWL6j"
   },
   "source": [
    "## Бонус-трек: Интуиция Ансамблирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LzCyka0WL6j"
   },
   "source": [
    "<img src='bias_variance.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YmalNyUpWL6j"
   },
   "source": [
    "### Феномен: «Мудрость толпы»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zBNEEABvWL6j"
   },
   "source": [
    "В 1906 году британский ученый сэр Фрэнсис Гальтон посещал выставку достижений животноводства, где случайно провел крайне важное наблюдение. \n",
    "\n",
    "На выставке проводился конкурс, в рамках которого всем желающим предлагалось на глаз угадать точный вес забитого быка. Побеждал тот, кто называл самое близкое к истинному значение. \n",
    "\n",
    "Полагая, что справиться с подобной задачей под силу только профессионалу, и чтобы доказать некомпетентность толпы, Гальтон посчитал среднее значение из почти восьми сотен догадок посетителей ярмарки. К удивлению ученого, толпа ошиблась меньше, чем на килограмм. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "dk_5bEeJWL6k"
   },
   "source": [
    "### Теорема Кондорсе о присяжных\n",
    "Если каждый член жюри присяжных имеет независимое мнение, и если вероятность правильного решения члена жюри больше 0.5, то тогда вероятность правильного решения присяжных в целом возрастает с увеличением количества членов жюри, и стремиться к единице. Если же вероятность быть правым у каждого из членов жюри меньше 0.5, то вероятность принятия правильного решения присяжными в целом монотонно уменьшается и стремится к нулю с увеличением количества присяжных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7A7x5KH8WL6k"
   },
   "source": [
    "$$\\mu = \\sum_{i=m}^{N}{}C^i_N p^i (1-p)^{N-i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1blncq-aWL6k"
   },
   "source": [
    "При $ p > 0,5 $, то $ \\mu > p $  \n",
    "  \n",
    "Если дополнительно добавить условие $N \\rightarrow \\infty $, то $ \\mu  \\rightarrow 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTqN7ac-WL6l"
   },
   "source": [
    "### Мы сделаем идеальный алгоритм?\n",
    "_Спойлер:_ Нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NXMLJcqWL6l"
   },
   "source": [
    "<img src='galton.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kp31m2h0WL6l"
   },
   "source": [
    "## Резюме\n",
    "\n",
    "- Обучение решающего дерева\n",
    "- Подбор оптимальных параметров для дерева\n",
    "- Дерево решений для задачи регрессии\n",
    "- Решение задачи классификация рукописных цифр \n",
    "- Оценка качества при многокласовой классификации\n",
    "\n",
    "\n",
    "**Ноутбук составлен по мотивам:**\n",
    "- [Открытый курс ODS по машинному обучению](https://habr.com/company/ods/blog/322626/)\n",
    "- [Блог](https://alexanderdyakonov.wordpress.com/2016/11/14/случайный-лес-random-forest/) Александра Дьяконова\n",
    "- [Курс](https://github.com/esokolov/ml-course-hse) Евгения Соколова по машинному обучению (материалы на GitHub)\n",
    "- Прикладное машинное обучение с помощью Scikit-Learn и TensorFlow // Орельен Жерон"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "trees_intro.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
